{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link medium**: https://medium.com/@oktayudha05/data-processing-dengan-apache-spark-7d965339179a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 1**: Buat DataFrame sederhana di Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/23 00:28:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+\n",
      "|Pegawai|Departmen|Gaji|\n",
      "+-------+---------+----+\n",
      "|  Yatno|    Sales|3000|\n",
      "|   Ifad|    Sales|4600|\n",
      "| Istoro|    Sales|4100|\n",
      "|   Agus| Direktur|3000|\n",
      "+-------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('tugas_pertemuan_6').getOrCreate()\n",
    "\n",
    "data = [('Yatno', 'Sales', 3000),\n",
    "        ('Ifad', 'Sales', 4600),\n",
    "        ('Istoro', 'Sales', 4100),\n",
    "        ('Agus', 'Direktur', 3000)]\n",
    "columns = ['Pegawai', 'Departmen', 'Gaji']\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 2**: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|Pegawai|Gaji|\n",
      "+-------+----+\n",
      "|  Yatno|3000|\n",
      "|   Ifad|4600|\n",
      "| Istoro|4100|\n",
      "|   Agus|3000|\n",
      "+-------+----+\n",
      "\n",
      "+-------+---------+----+\n",
      "|Pegawai|Departmen|Gaji|\n",
      "+-------+---------+----+\n",
      "|   Ifad|    Sales|4600|\n",
      "| Istoro|    Sales|4100|\n",
      "+-------+---------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|Departmen|avg(Gaji)|\n",
      "+---------+---------+\n",
      "|    Sales|   3900.0|\n",
      "| Direktur|   3000.0|\n",
      "+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('Pegawai', 'Gaji').show()\n",
    "df.filter(df['Gaji'] > 4000).show()\n",
    "df.groupBy('Departmen').avg('Gaji').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 3**: Eksplorasi bagaimana mengolah tipe data kompleks dalam Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+---------+\n",
      "|Pegawai|Departmen|Gaji|BonusGaji|\n",
      "+-------+---------+----+---------+\n",
      "|  Yatno|    Sales|3000|    300.0|\n",
      "|   Ifad|    Sales|4600|    460.0|\n",
      "| Istoro|    Sales|4100|    410.0|\n",
      "|   Agus| Direktur|3000|    300.0|\n",
      "+-------+---------+----+---------+\n",
      "\n",
      "+-------+---------+----+---------+---------------+\n",
      "|Pegawai|Departmen|Gaji|BonusGaji|TotalKompensasi|\n",
      "+-------+---------+----+---------+---------------+\n",
      "|  Yatno|    Sales|3000|    300.0|         3300.0|\n",
      "|   Ifad|    Sales|4600|    460.0|         5060.0|\n",
      "| Istoro|    Sales|4100|    410.0|         4510.0|\n",
      "|   Agus| Direktur|3000|    300.0|         3300.0|\n",
      "+-------+---------+----+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('BonusGaji', df['Gaji'] * 0.1).show()\n",
    "df.withColumn('TotalKompensasi', df['Gaji'] + df['BonusGaji']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 4**: Implementasikan window function untuk menghitung running totals atau rangkings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+---------+---------+\n",
      "|Pegawai|Departmen|Gaji|BonusGaji|Peringkat|\n",
      "+-------+---------+----+---------+---------+\n",
      "|   Agus| Direktur|3000|    300.0|        1|\n",
      "|  Yatno|    Sales|3000|    300.0|        1|\n",
      "| Istoro|    Sales|4100|    410.0|        2|\n",
      "|   Ifad|    Sales|4600|    460.0|        3|\n",
      "+-------+---------+----+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "windowSpec = Window.partitionBy('Departmen').orderBy('Gaji')\n",
    "df.withColumn('Peringkat', F.rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 5**: Buat ringkasan dari semua operasi yang telah dilakukan dan bagaimana teknik ini dapat diterapkan pada proyek data Anda.\n",
    "\n",
    "operasi yang kita lakukan: Pertama, kita membuat dataframe sederhana pada pyspark. Kemudian kita menggunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data. Selanjutnya kita mengolah tipe data kompleks dalam Spark DataFrames misalnya menambahkan BonusGaji dan TotalKompensasi. Dan terakhir mengimplementasikan window function.\n",
    "\n",
    "ini dapat kita implemetasikan untuk menyortir data data yang ada pada dataset projek yang akan kita kerjakan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
